{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "585c823e-6fe1-4fe8-adc5-0d51890db1e7",
   "metadata": {},
   "source": [
    "# Demo 3: DNN inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c56e44d-019a-4583-9ba8-691686cd851c",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this demo we show how different workflows can be used to optimize the performance of a user analysis, with a generic DNN model inference as an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b7568c-e128-49e8-99f5-d50973e657b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from python.event_selection import load_events\n",
    "from python.dnn_model import NeuralNet\n",
    "\n",
    "sources = [\"data\", \"ttbar\", \"dy\"]\n",
    "server = \"file:/depot/cms/purdue-af/demos/\"\n",
    "model_dir = \"/depot/cms/purdue-af/demos/\"\n",
    "dfs = {}\n",
    "\n",
    "features = ['mu1_pt', 'mu1_eta', 'mu2_pt', 'mu2_eta', 'dimuon_mass', 'met']\n",
    "\n",
    "# load datasets for inference\n",
    "for src in sources:\n",
    "    dfs[src] = load_events(f\"{server}/{src}.root\")[features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a5355-407e-4c57-a843-ba0a86be8624",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Connect to an existing cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92da10ce-0a7c-4d49-bd41-f3a4f003a685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from dask.distributed import Client\n",
    "\n",
    "# client = Client(\"tcp://127.0.0.1:42573\")\n",
    "# client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b7b365-e1c6-47aa-af94-9d6ddd5d4e94",
   "metadata": {},
   "source": [
    "### Or create a local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0bda4f-3cb7-443d-b0ce-d3170446c80a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster, Client\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c9940-6894-44ba-a4b3-387cbc1458ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Will use GPU for inference.\")\n",
    "else:\n",
    "    print(\"Will use CPUs for inference.\")\n",
    "\n",
    "def inference(inp):\n",
    "    label = inp[0]\n",
    "    df = inp[1]\n",
    "    #model_path=\"/depot/cms/purdue-af/triton/models/test-model/1/model.pt\"\n",
    "    model_path=model_dir+\"/model.ckpt\"\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model = NeuralNet(6, [16, 8], 1).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    df = torch.from_numpy(df.values).to(device).float()\n",
    "    scores = model(df) \n",
    "    scores = scores.cpu().detach().numpy()\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"output\": scores.ravel()\n",
    "    }\n",
    "\n",
    "scattered_data = client.scatter(list(dfs.items()))\n",
    "futures = client.map(inference, scattered_data)\n",
    "results = client.gather(futures)\n",
    "\n",
    "print(\"\\nInference outputs:\")\n",
    "for res in results:\n",
    "    print(res[\"label\"], res[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327803d1-3aac-47fa-b273-8245b2df80b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tritonclient.grpc as grpcclient\n",
    "\n",
    "#triton_address = '128.211.160.154:8001' #5gb\n",
    "#triton_address = '128.211.160.153:8001' #10gb\n",
    "#triton_address = '128.211.160.147:8001' #20gb\n",
    "triton_address = 'hammer-f000.rcac.purdue.edu:8001'\n",
    "\n",
    "print(f\"Connecting to Triton inference sever at {triton_address}\")\n",
    "\n",
    "keepalive_options = grpcclient.KeepAliveOptions(\n",
    "    keepalive_time_ms=2**31 - 1,\n",
    "    keepalive_timeout_ms=20000,\n",
    "    keepalive_permit_without_calls=False,\n",
    "    http2_max_pings_without_data=2\n",
    ")\n",
    "\n",
    "def inference_triton(inp):\n",
    "    # Create Triton client\n",
    "    try:\n",
    "        triton_client = grpcclient.InferenceServerClient(\n",
    "            url=triton_address,\n",
    "            verbose=False,\n",
    "            keepalive_options=keepalive_options\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Channel creation failed: \" + str(e))\n",
    "        sys.exit()\n",
    "    \n",
    "    label= inp[0]\n",
    "    df = inp[1]\n",
    "    \n",
    "    # Inputs and outputs should be compatible with model metadata\n",
    "    # stored in /depot/cms/purdue-af/triton/models/test-model/config.pbtxt\n",
    "    inputs = [grpcclient.InferInput('INPUT__0', df.shape, \"FP64\")]\n",
    "    outputs = [grpcclient.InferRequestedOutput('OUTPUT__0')]\n",
    "    \n",
    "    # Load input data\n",
    "    inputs[0].set_data_from_numpy(df.values)\n",
    "    \n",
    "    # Run inference on Triton server.\n",
    "    # Models are stored in /depot/cms/purdue-af/triton/models/\n",
    "    results = triton_client.infer(\n",
    "        model_name=\"test-model\",\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        headers={'test': '1'},\n",
    "    )\n",
    "\n",
    "    output = results.as_numpy('OUTPUT__0')\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"output\": output.flatten()\n",
    "    }\n",
    "\n",
    "# results = []\n",
    "# n = 1\n",
    "# for i in range(n):\n",
    "#     for label, df in dfs.items():\n",
    "#         results.append(inference_triton([label, df]))\n",
    "\n",
    "scattered_data = client.scatter(list(dfs.items()))\n",
    "futures = client.map(inference_triton, scattered_data)\n",
    "results = client.gather(futures)\n",
    "\n",
    "print(\"\\nInference outputs:\")\n",
    "for res in results:\n",
    "    print(res[\"label\"], res[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f337c-d48c-438f-b78b-a3837c3d5d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "bins = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(5,4))\n",
    "\n",
    "dnn = {res[\"label\"]: res[\"output\"] for res in results}\n",
    "\n",
    "plt.hist(dnn[\"dy\"], bins, alpha=0.3, label='dy', density=True)\n",
    "plt.hist(dnn[\"ttbar\"], bins, alpha=0.3, label='ttbar', density=True)\n",
    "plt.hist(dnn[\"data\"], bins, alpha=0.3, label='data', density=True)\n",
    "plt.xlabel('DNN Score')\n",
    "plt.ylabel('Events')\n",
    "leg = plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1b82aa-7765-49fc-b80c-85d124c71e51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 kernel [ML]",
   "language": "python",
   "name": "python3-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
